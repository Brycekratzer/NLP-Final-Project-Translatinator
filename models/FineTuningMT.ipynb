{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning MarianMT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import tqdm as tqdm\n",
    "from evaluate import load\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will walk through fine tuning the pre-existing MarianMT english to spanish model using the OpenSubtitles english and spanish text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will parse the data and a sentence in a row, with each column representing the spanish version and english version. We will then split the data appropriately. We will export as a csv for later usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Importing of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing English Sentences\n",
    "with open('../data/en-es/OpenSubtitles.en-es.en') as en_text:\n",
    "    english_sent = [line.strip() for line in en_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/en-es/OpenSubtitles.en-es.es') as es_text:\n",
    "    spanish_sent = [line.strip() for line in es_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert they are the same size\n",
    "len(english_sent), len(spanish_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.DataFrame({\n",
    "    'english': english_sent,\n",
    "    'spanish': spanish_sent\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.to_csv('en-es_Full_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.read_csv('../data/en-es_Full_Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data into 3 different sets. A test, validation, and test set. Each set will have english and spanish sentences. \n",
    "\n",
    "We will reserve 70% of our data for training, 10% for validation, and 20% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Training Data\n",
    "train_X = sentences['english'][:int(len(sentences) * .7)] # English only for input\n",
    "train_y = sentences['spanish'][:int(len(sentences) * .7)] # Spanish for target\n",
    "\n",
    "# Preparing Testing Data\n",
    "test_X = sentences['english'][int(len(sentences) * .7) : int(len(sentences) * .9)]\n",
    "test_y = sentences['spanish'][int(len(sentences) * .7) : int(len(sentences) * .9)]\n",
    "\n",
    "# Preparing Validation Data\n",
    "val_X = sentences['english'][int(len(sentences) * .9) : int(len(sentences))]\n",
    "val_y = sentences['spanish'][int(len(sentences) * .9) : int(len(sentences))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are converting our data into a datatype that is acceptable for the DataLoader class. The DataLoader class loads a certain amount of data to input to the model based on the batch size during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to tokenize our inputs so we can represent our text as a numerical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Helsinki-NLP/opus-mt-en-es'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class extends the Dataset class. What it does is returns the tokenized english sentence and spanish sentence for model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, labels, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = text\n",
    "        self.targets = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        target = self.targets[index]\n",
    "        \n",
    "        # Tokenize source text\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=128  # Adjust as needed\n",
    "        )\n",
    "        \n",
    "        # Tokenize target text\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(\n",
    "                target,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                max_length=128  # Adjust as needed\n",
    "            )\n",
    "        \n",
    "        # Remove batch dimension created by return_tensors=\"pt\"\n",
    "        source = inputs['input_ids'].squeeze()\n",
    "        targets = labels['input_ids'].squeeze()\n",
    "        \n",
    "        return {\n",
    "            'ids': source,\n",
    "            'targets': targets\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [37116, 35, 52, 806, 4452, 10165, 127, 67, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [35010, 12, 52, 806, 4452, 10165, 127, 67, 0]}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs = tokenizer(train_X[0], text_target=train_y[0], truncation=True)\n",
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting our data into a torch tensor\n",
    "train_X = np.array(train_X)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "test_X = np.array(test_X)\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "val_X = np.array(val_X)\n",
    "val_y = np.array(val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataset based on our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MultiLabelDataset(train_X, train_y, tokenizer)\n",
    "test_data = MultiLabelDataset(test_X, test_y, tokenizer)\n",
    "val_data = MultiLabelDataset(val_X, val_y, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# Create DataLoaders \n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be fine-tuning the MarianMT with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Helsinki-NLP/opus-mt-en-es'\n",
    "model = MarianMTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "model.to(device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_loader, optimizer):\n",
    "    model.train()\n",
    "    for data in tqdm.tqdm(training_loader):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=ids, decoder_input_ids=targets)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, val_loader, tokenizer, bertscore):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm.tqdm(val_loader):\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            generated_ids = model.generate(input_ids = ids)\n",
    "            \n",
    "            predictions = tokenizer.batch_decode(generated_ids)\n",
    "            references = tokenizer.batch_decode(targets)\n",
    "            \n",
    "            all_predictions.append(predictions)\n",
    "            all_references.append(references)\n",
    "        results = bertscore.compute(predictions=all_predictions, references=all_references, lang='es')\n",
    "    return results['f1']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4614857 [00:00<?, ?it/s]/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "  0%|          | 0/4614857 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb Cell 35\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m bertscore \u001b[39m=\u001b[39m load(\u001b[39m\"\u001b[39m\u001b[39mbertscore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_EPOCH):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     loss \u001b[39m=\u001b[39m train(model, train_loader, optim)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     f1_score \u001b[39m=\u001b[39m val(model, val_loader, tokenizer, bertscore)\n",
      "\u001b[1;32m/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb Cell 35\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39mids, decoder_input_ids\u001b[39m=\u001b[39mtargets)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.AdamW(model.parameters(), lr=.00002)\n",
    "NUM_EPOCH = 3\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    loss = train(model, train_loader, optim)\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss}')\n",
    "    f1_score = val(model, val_loader, tokenizer, bertscore)\n",
    "    print(f'Epoch: {epoch+1}, BERTScore F1: {f1_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
