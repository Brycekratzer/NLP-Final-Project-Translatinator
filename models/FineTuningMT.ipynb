{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning MarianMT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import tqdm as tqdm\n",
    "from evaluate import load\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will walk through fine tuning the pre-existing MarianMT english to spanish model using the OpenSubtitles english and spanish text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will parse the data and a sentence in a row, with each column representing the spanish version and english version. We will then split the data appropriately. We will export as a csv for later usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Importing of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing English sentences\n",
    "with open('../data/en-es/OpenSubtitles.en-es.en') as en_text:\n",
    "    english_sent = [line.strip() for line in en_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/en-es/OpenSubtitles.en-es.es') as es_text:\n",
    "    spanish_sent = [line.strip() for line in es_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert they are the same size\n",
    "len(english_sent), len(spanish_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.DataFrame({\n",
    "    'english': english_sent,\n",
    "    'spanish': spanish_sent\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.to_csv('en-es_Full_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.read_csv('../data/en-es_Full_Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data into 3 different sets. A test, validation, and test set. Each set will have english and spanish sentences. \n",
    "\n",
    "We will reserve 70% of our data for training, 10% for validation, and 20% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_sample = sentences[:int(len(sentences)*.000001)]\n",
    "len(sentences_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Training Data\n",
    "train_X = sentences['english'][:int(len(sentences) * .7)] # English only for input\n",
    "train_y = sentences['spanish'][:int(len(sentences) * .7)] # Spanish for target\n",
    "\n",
    "# Preparing Testing Data\n",
    "test_X = sentences['english'][int(len(sentences) * .7) : int(len(sentences) * .9)]\n",
    "test_y = sentences['spanish'][int(len(sentences) * .7) : int(len(sentences) * .9)]\n",
    "\n",
    "# Preparing Validation Data\n",
    "val_X = sentences['english'][int(len(sentences) * .9) : int(len(sentences))]\n",
    "val_y = sentences['spanish'][int(len(sentences) * .9) : int(len(sentences))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sample size to test code\n",
    "\n",
    "# Preparing Training Data\n",
    "train_X = sentences_sample['english'][:int(len(sentences_sample) * .7)] # English only for input\n",
    "train_y = sentences_sample['spanish'][:int(len(sentences_sample) * .7)] # Spanish for target\n",
    "\n",
    "# Preparing Testing Data\n",
    "test_X = sentences_sample['english'][int(len(sentences_sample) * .7) : int(len(sentences_sample) * .9)]\n",
    "test_y = sentences_sample['spanish'][int(len(sentences_sample) * .7) : int(len(sentences_sample) * .9)]\n",
    "\n",
    "# Preparing Validation Data\n",
    "val_X = sentences_sample['english'][int(len(sentences_sample) * .9) : int(len(sentences_sample))]\n",
    "val_y = sentences_sample['spanish'][int(len(sentences_sample) * .9) : int(len(sentences_sample))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are converting our data into a datatype that is acceptable for the DataLoader class. The DataLoader class loads a certain amount of data to input to the model based on the batch size during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to tokenize our inputs so we can represent our text as a numerical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Helsinki-NLP/opus-mt-en-es'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class extends the Dataset class. What it does is returns the tokenized english sentence and spanish sentence for model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, labels, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = text\n",
    "        self.targets = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        target = self.targets[index]\n",
    "        \n",
    "        # Tokenize source text\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128 , # Adjust as needed\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Tokenize target text\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(\n",
    "                target,\n",
    "                truncation=True,\n",
    "                max_length=128, # Adjust as needed\n",
    "                pad_to_max_length=True,\n",
    "                return_tensors=None\n",
    "            )\n",
    "        \n",
    "        # Remove batch dimension created by return_tensors=\"pt\"\n",
    "        source = inputs['input_ids']\n",
    "        targets = labels['input_ids']\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(source, dtype=torch.long),\n",
    "            'targets': torch.tensor(targets, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = tokenizer.encode_plus(train_X[0], truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting our data into a torch tensor\n",
    "train_X = np.array(train_X)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "test_X = np.array(test_X)\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "val_X = np.array(val_X)\n",
    "val_y = np.array(val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataset based on our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MultiLabelDataset(train_X, train_y, tokenizer)\n",
    "test_data = MultiLabelDataset(test_X, test_y, tokenizer)\n",
    "val_data = MultiLabelDataset(val_X, val_y, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# Create DataLoaders \n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be fine-tuning the MarianMT with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Helsinki-NLP/opus-mt-en-es'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "base_model = MarianMTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "model.to(device)    \n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_loader, optimizer):\n",
    "    model.train()\n",
    "    for data in training_loader:\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=ids, decoder_input_ids=targets)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, val_loader, tokenizer, bertscore):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm.tqdm(val_loader):\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            generated_ids = model.generate(input_ids = ids)\n",
    "            \n",
    "            predictions = tokenizer.batch_decode(generated_ids, kip_special_tokens=True)[0]\n",
    "            references = tokenizer.batch_decode(targets, skip_special_tokens=True)[0]\n",
    "            \n",
    "            all_predictions.append(predictions)\n",
    "            all_references.append(references)\n",
    "        results = bertscore.compute(predictions=all_predictions, references=all_references, lang='es')\n",
    "    return results['f1']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, tokenizer, bertscore):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm.tqdm(test_loader):\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            generated_ids = model.generate(input_ids = ids)\n",
    "            \n",
    "            predictions = tokenizer.batch_decode(generated_ids, kip_special_tokens=True)[0]\n",
    "            references = tokenizer.batch_decode(targets, skip_special_tokens=True)[0]\n",
    "            \n",
    "            all_predictions.append(predictions)\n",
    "            all_references.append(references)\n",
    "        results = bertscore.compute(predictions=all_predictions, references=all_references, lang='es')\n",
    "    return results['f1'], all_predictions, all_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb Cell 38\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m bertscore \u001b[39m=\u001b[39m load(\u001b[39m\"\u001b[39m\u001b[39mbertscore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_EPOCH):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train(model, train_loader, optim)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     f1_score \u001b[39m=\u001b[39m val(model, val_loader, tokenizer, bertscore)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, BERTScore F1: \u001b[39m\u001b[39m{\u001b[39;00mf1_score\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb Cell 38\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(model, training_loader, optimizer):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m training_loader:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         ids \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device, dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlong)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         targets \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device, dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlong)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    709\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets[index]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Tokenize source text\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     text,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m , \u001b[39m# Adjust as needed\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     pad_to_max_length\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Tokenize target text\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X52sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mas_target_tokenizer():\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2868\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2866\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2867\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2868\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[1;32m   2869\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2870\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2928\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2925\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   2927\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2928\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2929\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2930\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2931\u001b[0m     )\n\u001b[1;32m   2933\u001b[0m \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2934\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2935\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2936\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2937\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "optim = torch.optim.AdamW(model.parameters(), lr=.00002)\n",
    "NUM_EPOCH = 3\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    train(model, train_loader, optim)\n",
    "    f1_score = val(model, val_loader, tokenizer, bertscore)\n",
    "    print(f'Epoch: {epoch+1}, BERTScore F1: {f1_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will evalute our model. We will compare our fine tuned model to our base model using the BERTScore which finds the cosine similarity between two translated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_based_model, pred, ref = test(base_model, test_loader, tokenizer, bertscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(f1_score_based_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
