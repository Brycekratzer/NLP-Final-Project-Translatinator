{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning MarianMT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm as tqdm\n",
    "from evaluate import load\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will walk through fine tuning the pre-existing MarianMT english to spanish model using the OpenSubtitles english and spanish text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will parse the data and a sentence in a row, with each column representing the spanish version and english version. We will then split the data appropriately. We will export as a csv for later usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Importing of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing English sentences\n",
    "with open('../data/en-es/OpenSubtitles.en-es.en') as en_text:\n",
    "    english_sent = [line.strip() for line in en_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/en-es/OpenSubtitles.en-es.es') as es_text:\n",
    "    spanish_sent = [line.strip() for line in es_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert they are the same size\n",
    "len(english_sent), len(spanish_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.DataFrame({\n",
    "    'english': english_sent,\n",
    "    'spanish': spanish_sent\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.to_csv('en-es_Full_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.read_csv('../data/en-es_Full_Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data into 3 different sets. A test, validation, and test set. Each set will have english and spanish sentences. \n",
    "\n",
    "We will reserve 70% of our data for training, 10% for validation, and 20% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10548"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_sample = sentences[:int(len(sentences)*.0001)]\n",
    "len(sentences_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Training Data\n",
    "train_X = sentences['english'][:int(len(sentences) * .7)] # English only for input\n",
    "train_y = sentences['spanish'][:int(len(sentences) * .7)] # Spanish for target\n",
    "\n",
    "# Preparing Testing Data\n",
    "test_X = sentences['english'][int(len(sentences) * .7) : int(len(sentences) * .9)]\n",
    "test_y = sentences['spanish'][int(len(sentences) * .7) : int(len(sentences) * .9)]\n",
    "\n",
    "# Preparing Validation Data\n",
    "val_X = sentences['english'][int(len(sentences) * .9) : int(len(sentences))]\n",
    "val_y = sentences['spanish'][int(len(sentences) * .9) : int(len(sentences))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sample size to test code\n",
    "\n",
    "# Preparing Training Data\n",
    "train_X = sentences_sample['english'][:int(len(sentences_sample) * .7)] # English only for input\n",
    "train_y = sentences_sample['spanish'][:int(len(sentences_sample) * .7)] # Spanish for target\n",
    "\n",
    "# Preparing Testing Data\n",
    "test_X = sentences_sample['english'][int(len(sentences_sample) * .7) : int(len(sentences_sample) * .9)]\n",
    "test_y = sentences_sample['spanish'][int(len(sentences_sample) * .7) : int(len(sentences_sample) * .9)]\n",
    "\n",
    "# Preparing Validation Data\n",
    "val_X = sentences_sample['english'][int(len(sentences_sample) * .9) : int(len(sentences_sample))]\n",
    "val_y = sentences_sample['spanish'][int(len(sentences_sample) * .9) : int(len(sentences_sample))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are converting our data into a datatype that is acceptable for the DataLoader class. The DataLoader class loads a certain amount of data to input to the model based on the batch size during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to tokenize our inputs so we can represent our text as a numerical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Helsinki-NLP/opus-mt-en-es'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting our data into a torch tensor\n",
    "train_X = np.array(train_X)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "test_X = np.array(test_X)\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "val_X = np.array(val_X)\n",
    "val_y = np.array(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X, y, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        # Creating a dataset of inputs to model, and their outputs\n",
    "        for data_X, data_y in zip(X, y):\n",
    "            \n",
    "            # Handles if the input file has Non-string like objects\n",
    "            if isinstance(data_X, str) and isinstance(data_y, str):\n",
    "                \n",
    "                # Tokenize text\n",
    "                inputs = tokenizer(data_X, max_length=128, padding='max_length', truncation=True)\n",
    "                with tokenizer.as_target_tokenizer():\n",
    "                    targets = tokenizer(data_y, max_length=128, padding='max_length', truncation=True)\n",
    "                \n",
    "                # Add tokenized text to our data sets\n",
    "                self.input_ids.append(inputs['input_ids'])\n",
    "                self.target_ids.append(targets['input_ids'])\n",
    "        \n",
    "        # Convert to tensors\n",
    "        self.input_ids = torch.tensor(self.input_ids)\n",
    "        self.target_ids = torch.tensor(self.target_ids)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'input': self.input_ids[index],\n",
    "            'target': self.target_ids[index]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(train_X, train_y, tokenizer)\n",
    "test_dataset = TextDataset(test_X, test_y, tokenizer)\n",
    "val_dataset = TextDataset(val_X, val_y, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataset based on our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# Create DataLoaders \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be fine-tuning the MarianMT with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Helsinki-NLP/opus-mt-en-es'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "base_model = MarianMTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "model.to(device)    \n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in tqdm.tqdm(training_loader):\n",
    "        ids = data['input'].to(device, dtype = torch.long)\n",
    "        targets = data['target'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, labels=targets)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    return total_loss / len(training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, val_loader, tokenizer, bertscore):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm.tqdm(val_loader):\n",
    "            targets = data['target'].to(device, dtype = torch.long)\n",
    "            ids = data['input'].to(device, dtype = torch.long)\n",
    "            generated_ids = model.generate(input_ids = ids)\n",
    "            \n",
    "            predictions = tokenizer.batch_decode(generated_ids, kip_special_tokens=True)[0]\n",
    "            references = tokenizer.batch_decode(targets, skip_special_tokens=True)[0]\n",
    "            \n",
    "            all_predictions.append(predictions)\n",
    "            all_references.append(references)\n",
    "        results = bertscore.compute(predictions=all_predictions, references=all_references, device=device, lang='es')\n",
    "    return results['f1']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, tokenizer, bertscore):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm.tqdm(test_loader):\n",
    "            targets = data['target'].to(device, dtype = torch.long)\n",
    "            ids = data['input'].to(device, dtype = torch.long)\n",
    "            generated_ids = model.generate(input_ids = ids)\n",
    "            \n",
    "            predictions = tokenizer.batch_decode(generated_ids, kip_special_tokens=True)[0]\n",
    "            references = tokenizer.batch_decode(targets, skip_special_tokens=True)[0]\n",
    "            \n",
    "            all_predictions.append(predictions)\n",
    "            all_references.append(references)\n",
    "        results = bertscore.compute(predictions=all_predictions, references=all_references, lang='es', device=device)\n",
    "    return results['f1'], all_predictions, all_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [03:31<00:00,  2.18it/s]\n",
      "100%|██████████| 66/66 [02:19<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \n",
      "BERTScore F1: [0.6230504512786865, 0.6635830998420715, 0.7441152334213257, 0.6927334070205688, 0.6215762495994568, 0.6076961755752563, 0.6257904767990112, 0.6419216394424438, 0.811438262462616, 0.7711580395698547, 0.6299823522567749, 0.523600697517395, 0.812808632850647, 0.6375217437744141, 0.6554610133171082, 0.7655702233314514, 0.6767503023147583, 0.617131769657135, 0.6805600523948669, 0.6771442890167236, 0.6619366407394409, 0.7316170930862427, 0.6248447895050049, 0.5699236989021301, 0.5666306018829346, 0.6994898915290833, 0.6539263129234314, 0.7212305665016174, 0.6997771263122559, 0.5941764116287231, 0.6796661615371704, 0.47129371762275696, 0.6859456896781921, 0.6596882343292236, 0.6611955165863037, 0.7750704288482666, 0.6977666616439819, 0.5598645806312561, 0.8798046708106995, 0.5718146562576294, 0.5152043104171753, 0.552919328212738, 0.6522799134254456, 0.6007460355758667, 0.6630284786224365, 0.7783555388450623, 0.5862745046615601, 0.7419335246086121, 0.6409316658973694, 0.7009351849555969, 0.6844218969345093, 0.7492485642433167, 0.6837075352668762, 0.6637378334999084, 0.5419084429740906, 0.598418116569519, 0.6374188661575317, 0.7885209918022156, 0.6521531939506531, 0.5691501498222351, 0.5650396347045898, 0.6751358509063721, 0.6046825647354126, 0.6369689106941223, 0.7070884704589844, 0.6092042922973633]\n",
      "Training Loss: 4.348066864075599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 20/462 [00:10<03:56,  1.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb Cell 36\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m bertscore \u001b[39m=\u001b[39m load(\u001b[39m\"\u001b[39m\u001b[39mbertscore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_EPOCH):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     loss \u001b[39m=\u001b[39m train(model, train_loader, optim)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     f1_score \u001b[39m=\u001b[39m val(model, val_loader, tokenizer, bertscore)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X53sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mBERTScore F1: \u001b[39m\u001b[39m{\u001b[39;00mf1_score\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mTraining Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb Cell 36\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39mids, labels\u001b[39m=\u001b[39mtargets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X53sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X53sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X53sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brycekratzer/BoiseStateWork/SPRING25/NLP_Project_With_Branch/NLP-Final-Project-Translatinator/models/FineTuningMT.ipynb#X53sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    628\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_25/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         t_outputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    825\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optim = torch.optim.AdamW(model.parameters(), lr=.0001)\n",
    "NUM_EPOCH = 3\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    loss = train(model, train_loader, optim)\n",
    "    \n",
    "    f1_score = val(model, val_loader, tokenizer, bertscore)\n",
    "    print(f'Epoch: {epoch+1} \\nBERTScore F1: {f1_score}\\nTraining Loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will evalute our model. We will compare our fine tuned model to our base model using the BERTScore which finds the cosine similarity between two translated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_based_model = test(base_model, test_loader, tokenizer, bertscore)\n",
    "f1_score_finetuned = test(model, test_loader, tokenizer, bertscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(f1_score_based_model), np.mean(f1_score_finetuned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
